We build two attention units on top of the multi-head attention to handle the multimodal input features for VQA, namely the self-attention (SA) unit and the guided-attention (GA) unit. The SA unit (see Figure 2a) is composed of a multi-head attention layer and a pointwise feed-forward layer. Taking one group of input features $X = [x_1; \dots; x_m] \in \mathbb{R}^{m \times d_x}$, the multi-head attention learns the pairwise relationship between the paired sample $<x_i, x_j>$ within $X$ and outputs the attended output features $Z \in \mathbb{R}^{m \times d}$ by weighted summation of all the instances in $X$. The feedforward layer takes the output features of the multi-head attention layer, and further transforms them through two fully-connected layers with ReLU activation and dropout $\text{(FC(4}d\text{)-ReLU-Dropout(0.1)-FC(}d\text{))}$. Moreover, residual connection [12] followed by layer normalization [3] is applied to the outputs of the two layers to facilitate optimization. The GA unit (see Figure 2b) takes two groups of input features $X \in \mathbb{R}^{m \times d_x}$ and $Y = [y_1; \dots; y_n] \in \mathbb{R}^{n \times d_y}$, where $Y$ guides the attention learning for $X$. Note that the shapes of $X$ and $Y$ are flexible, so they can be used to represent the features for different modalities (e.g., questions and images). The GA unit models the pairwise relationship between the each paired sample $< x_i, y_j >$ from $X$ and $Y$ , respectively.